{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "AUTO",
   "gpuType": "T4",
   "accelerator": "GPU"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Whisper Model for Nepali Speech Recognition on Google Colab\n",
    "\n",
    "This notebook provides a complete environment to fine-tune an OpenAI Whisper model (base version) on the Common Voice Nepali dataset. It's configured to run on Google Colab's GPU.\n",
    "\n",
    "**Before you start:**\n",
    "\n",
    "1.  **Upload your project folder to Google Drive:** Ensure your `OpenAiWhisperNepali` folder (or a zip of it) is uploaded to your Google Drive. If it's a zip, make sure it's named `OpenAiWhisperNepali.zip` or adjust the `!unzip` command accordingly.\n",
    "2.  **Verify your dataset structure:** Inside `OpenAiWhisperNepali`, you should have a `mozilla/ne` directory, and inside `ne`, there should be `clips/`, `train.tsv`, and `dev.tsv`.\n",
    "3.  **Ensure `requirements.txt` is present:** A `requirements.txt` file should be in your `OpenAiWhisperNepali` folder. A basic one would include:\n",
    "    ```\n",
    "    transformers\n",
    "    datasets\n",
    "    accelerate\n",
    "    torch\n",
    "    evaluate\n",
    "    ```\n",
    "\n",
    "**Steps to run this notebook:**\n",
    "\n",
    "1.  **Run all cells sequentially.**\n",
    "2.  **Grant Google Drive access** when prompted.\n",
    "3.  **Monitor the output** for training progress."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "e7c266a2-e64e-4e68-98f5-93c6607231ce",
    "outputId": "97e682d3-87a1-432d-9445-562a265b404d"
   },
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "50e7a270-13f5-472d-953e-526978d381c1",
    "outputId": "34436814-7253-4876-8f35-e10c73d93707"
   },
   "source": [
    "# 2. Navigate to your project directory and unzip (if applicable)\n",
    "# Adjust 'OpenAiWhisperNepali.zip' and the target directory if your file/folder names differ.\n",
    "\n",
    "# Change to the directory where your zip file is located (e.g., MyDrive root)\n",
    "%cd /content/drive/MyDrive/\n",
    "\n",
    "# Unzip the project folder. '-q' for quiet output.\n",
    "!unzip -q OpenAiWhisperNepali.zip\n",
    "\n",
    "# Navigate into the unzipped project directory\n",
    "%cd /content/drive/MyDrive/OpenAiWhisperNepali/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "76b92a2a-5f21-4999-923f-42a1768b753a",
    "outputId": "50323381-081b-4171-872f-51532f6a735c"
   },
   "source": [
    "# 3. Install dependencies from requirements.txt\n",
    "# This ensures all necessary libraries are installed in the Colab environment.\n",
    "!python3 -m pip install -r requirements.txt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "56f8f8f8-f8f8-f8f8-f8f8-f8f8f8f8f8f8",
    "outputId": "43214321-4321-4321-4321-432143214321"
   },
   "source": [
    "# 4. Create the index.py script\n",
    "# This cell writes the complete Python script to a file named 'index.py'\n",
    "# in the current working directory (your project folder).\n",
    "%%writefile index.py\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset, Audio, Value\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# --- Configuration ---\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "common_voice_dir = os.path.join(script_dir, \"mozilla\", \"ne\")\n",
    "audio_base_path = os.path.join(common_voice_dir, \"clips\")\n",
    "model_name = \"openai/whisper-base\" # Changed to 'base' for memory efficiency\n",
    "\n",
    "# --- 1. Load Pre-trained Whisper Model and Processor ---\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=\"ne\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (CUDA or MPS) if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"CUDA is available. Model moved to GPU.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    model = model.to(\"mps\")\n",
    "    print(\"MPS is available. Model moved to MPS device for Apple Silicon acceleration.\")\n",
    "else:\n",
    "    print(\"No GPU acceleration (CUDA/MPS) available. Model will run on CPU.\")\n",
    "\n",
    "# --- 2. Load Common Voice Dataset from Local TSV files ---\n",
    "train_tsv_path = os.path.join(common_voice_dir, \"train.tsv\")\n",
    "dev_tsv_path = os.path.join(common_voice_dir, \"dev.tsv\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\": train_tsv_path,\n",
    "    \"validation\": dev_tsv_path,\n",
    "}\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=data_files,\n",
    "        delimiter=\"\\t\",\n",
    "        column_names=[\"client_id\", \"path\", \"sentence_id\", \"sentence\", \"sentence_domain\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accents\", \"variant\", \"locale\", \"segment\"],\n",
    "        skiprows=1\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find Common Voice TSV files. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Data Preprocessing: Filtering and Column Management ---\n",
    "dataset = dataset.cast_column(\"sentence\", Value(\"string\"))\n",
    "\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    dataset[split] = dataset[split].filter(\n",
    "        lambda x: x[\"sentence\"] is not None and len(x[\"sentence\"].strip()) > 0,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=1\n",
    "    )\n",
    "    dataset[split] = dataset[split].filter(\n",
    "        lambda x: x[\"path\"] is not None and len(str(x[\"path\"]).strip()) > 0,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "def create_full_audio_path_column_batched(batch, audio_dir):\n",
    "    return {\"audio_path\": [os.path.join(audio_dir, str(p)) for p in batch[\"path\"]]}\n",
    "\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    if len(dataset[split]) > 0:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            lambda batch: create_full_audio_path_column_batched(batch, audio_base_path),\n",
    "            batched=True,\n",
    "            num_proc=os.cpu_count(),\n",
    "            load_from_cache_file=False\n",
    "        )\n",
    "        cols_to_remove = [\"path\"] + [col for col in dataset[split].column_names if col.startswith(\"__index_level_\")]\n",
    "        cols_to_remove_existing = [col for col in cols_to_remove if col in dataset[split].column_names]\n",
    "        if cols_to_remove_existing:\n",
    "            dataset[split] = dataset[split].remove_columns(cols_to_remove_existing)\n",
    "\n",
    "# --- Rename columns for Whisper model compatibility ---\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    if len(dataset[split]) > 0:\n",
    "        if \"audio_path\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].rename_column(\"audio_path\", \"audio\")\n",
    "        if \"sentence\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].rename_column(\"sentence\", \"text\")\n",
    "\n",
    "# --- Cast 'audio' column to Audio feature ---\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    if len(dataset[split]) > 0 and \"audio\" in dataset[split].column_names:\n",
    "        dataset[split] = dataset[split].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# --- 3. Prepare the dataset for Whisper (Feature Extraction and Tokenization) ---\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "if len(dataset[\"train\"]) > 0:\n",
    "    tokenized_dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=os.cpu_count(),\n",
    "        load_from_cache_file=False\n",
    "    )\n",
    "else:\n",
    "    tokenized_dataset = dataset\n",
    "\n",
    "# --- 4. Define Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# --- 5. Define Evaluation Metric (Word Error Rate - WER) ---\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# --- 6. Configure Training Arguments and Trainer ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-fine-tuned-nepali-cv\",\n",
    "    per_device_train_batch_size=2, # Reduced for memory\n",
    "    gradient_accumulation_steps=8, # Increased to maintain effective batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=False, # Disabled to avoid graph issues on MPS\n",
    "    fp16=False, # Disabled as MPS does not support FP16 directly\n",
    "    eval_strategy=\"steps\", # Corrected keyword\n",
    "    per_device_eval_batch_size=2, # Reduced for memory\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = None\n",
    "if len(tokenized_dataset[\"train\"]) > 0:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"] if len(tokenized_dataset[\"validation\"]) > 0 else None,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor.tokenizer, # FutureWarning: Use `processing_class` instead in v5.0.0\n",
    "    )\n",
    "\n",
    "# --- 7. Start Training ---\n",
    "if trainer:\n",
    "    trainer.train()\n",
    "    processor.save_pretrained(\"./whisper-fine-tuned-nepali-cv\")\n",
    "    model.save_pretrained(\"./whisper-fine-tuned-nepali-cv\")\n",
    "\n",
    "# --- 8. Inference (Testing Your Fine-tuned Model) ---\n",
    "model_path_inference = \"./whisper-fine-tuned-nepali-cv\"\n",
    "\n",
    "pipe = None\n",
    "try:\n",
    "    pipe = pipeline(\"automatic-speech-recognition\", model=model_path_inference, device=0 if torch.cuda.is_available() else -1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pipeline for inference: {e}\")\n",
    "\n",
    "if pipe:\n",
    "    sample_entry = None\n",
    "    if \"test\" in dataset and len(dataset[\"test\"]) > 0:\n",
    "        sample_entry = dataset[\"test\"][0]\n",
    "    elif len(dataset[\"validation\"]) > 0:\n",
    "        sample_entry = dataset[\"validation\"][0]\n",
    "\n",
    "    if sample_entry:\n",
    "        sample_audio_data = sample_entry[\"audio\"]\n",
    "        actual_text = sample_entry[\"text\"]\n",
    "        result = pipe(sample_audio_data[\"array\"], sampling_rate=sample_audio_data[\"sampling_rate\"], generate_kwargs={\"language\": \"ne\", \"task\": \"transcribe\"})\n",
    "        print(f\"Original Text: {actual_text}\")\n",
    "        print(f\"Transcribed Text: {result['text']}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}